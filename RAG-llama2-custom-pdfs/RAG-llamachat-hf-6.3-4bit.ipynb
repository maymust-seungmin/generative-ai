{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7dc7e39-dd9a-43e3-8d9a-f77af583004c",
   "metadata": {},
   "source": [
    "### Dell Technologies Proof of Concept - RAG Llama2-Chat-7b-hf 4 bit PDF Retrieval Knowledgebase Assistant\n",
    "- Model:  llama2-7b-chat-hf  (4 bit)\n",
    "- Vector database:  Chroma db\n",
    "- Chain:  Langchain retrievalQAchainwithSources, huggingface pipeline\n",
    "- GUI:  Gradio interface (not with blocks)\n",
    "- Workload:  RAG PDF knowledgebase\n",
    "- limited PDF file dataset from https://infohub.delltechnologies.com/\n",
    "\n",
    "Features in Additional Inputs:\n",
    "- Change persona ad hoc with adjustable system prompt\n",
    "- Change model parameters with sliders (temp., top-p, top-k, max_tokens)\n",
    "- Memory is intact and conversational using chat_history key\n",
    "- Create all types of content such as email, product description, product comparison tables etc.\n",
    "- Directly query / summarize a document given the title\n",
    "\n",
    "Note: The software and sample files are provided “as is” and are to be used only in conjunction with this POC application. They should not be used in production and are provided without warranty or guarantees. Please use them at your own discretion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef99d2-76ef-45c1-af70-bda234cb8fed",
   "metadata": {},
   "source": [
    "<img src=\"images/RAG-diagram-dell-technologies.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbcd722",
   "metadata": {},
   "source": [
    "### Huggingface tools\n",
    "\n",
    "You will need to at least log in once to get the hub for tools and the embedding model.  After that you can comment this section out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4d542-e238-4948-ade3-efb972c78cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "token = 'YOUR_TOKEN'\n",
    "login(token=token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb3e4a-af6a-4852-98a0-97afaff2924c",
   "metadata": {},
   "source": [
    "### Assign GPU environment vars and ID order\n",
    "\n",
    "NOTE:  to change which GPU you want visible, simply change the CUDA VISIBLE DEVICES ID to the GPU you prefer. \n",
    "This method guarantees no confusion or misplaced workloads on any GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf410b-3b71-4299-8bc9-0b6fb09f4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THESE VARIABLES MUST APPEAR BEFORE TORCH OR CUDA IS IMPORTED\n",
    "## set visible GPU devices and order of IDs to the PCI bus order\n",
    "## target the L40s that is on ID 1\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"   \n",
    "\n",
    "## this integer corresponds to the ID of the GPU, for multiple GPU use \"0,1,2,3\"...\n",
    "## to disable all GPUs, simply put empty quotes \"\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846fa67-69d7-478e-995e-d16a5ba98f72",
   "metadata": {},
   "source": [
    "### Investigate our GPU and CUDA environment\n",
    "\n",
    "NOTE:  If you are only using 1 single GPU in the visibility settings above, then the active CUDA device will always be 0 since it is the only GPU seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaec1edd-9c18-4b1f-a0e8-e3ed03b3141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from subprocess import call\n",
    "print('_____Python, Pytorch, Cuda info____')\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA RUNTIME API VERSION')\n",
    "#os.system('nvcc --version')\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('_____nvidia-smi GPU details____')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('_____Device assignments____')\n",
    "print('Number CUDA Devices:', torch.cuda.device_count())\n",
    "print ('Current cuda device: ', torch.cuda.current_device(), ' **May not correspond to nvidia-smi ID above, check visibility parameter')\n",
    "print(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bae0d5-b6ef-425b-9e29-13393a839bcf",
   "metadata": {},
   "source": [
    "### Assign single GPU to device variable\n",
    "\n",
    "This command assigns GPU ID 0 to the DEVICE variable called \"cuda:0\" if pytorch can actually reach and speak with the GPU using cuda language.  Else it will use the cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c26efcc3-328c-49fe-9fe6-bbdbec423b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m DEVICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(DEVICE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639b742-c004-4bcd-9dd6-298a0fc499d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "#from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from pdf2image import convert_from_path\n",
    "from transformers import AutoTokenizer, pipeline, TextIteratorStreamer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7481a10-af9f-4154-bddb-e429d4361ecf",
   "metadata": {},
   "source": [
    "### Clear GPU memory from any previous runs\n",
    "- assume Nvidia drivers installed\n",
    "- When running notebooks over and over again, often much of the memory is still in the GPU memory allocated cache.  Depending on the size of the GPU, this might cause out of memory issues during the next run.  It is advised to clear out the cache, or restart the kernel.\n",
    "- here we see multiple GPUs, the memory usage, any running processes and our CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cfe3b9-5c65-4708-ad97-5bb3a04d2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6559d3-b08a-4ecb-a00c-cddba9f958e2",
   "metadata": {},
   "source": [
    "### Clear the previous run vector database\n",
    "\n",
    "This is optional, the vector db will be rebuilt.  For a completely fresh run you can delete the local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4084ec3-db2b-475d-8e4b-5ea3f980879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove chroma vector db local db folder from previous run\n",
    "!rm -rf \"vector-db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee7825-5a61-4950-aa5a-c1a905592d1b",
   "metadata": {},
   "source": [
    "### Prepare data from knowledge base\n",
    "\n",
    "- load the pdf files\n",
    "- use an instruct model to intelligently split the content into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e497d-7740-4162-bb56-80c6845e06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs-dell-infohub\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf945d8-2c0d-48d3-b08f-31f71fd61d34",
   "metadata": {},
   "source": [
    "### Use Instruct model to split text intelligently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad384a01-6147-4251-abfe-2329b7bb3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a0824-cb0b-476a-83aa-b667481be306",
   "metadata": {},
   "source": [
    "### Chunk text\n",
    "\n",
    "<b>chunk size large</b>:  If you want to provide large text overviews and summaries in your responses - appropriate for content creation tasks - then a large chunk size is helpful.  800 or higher.\n",
    "\n",
    "<b>chunk size small</b>:  If you are looking for specific answers based on extracted content from your knowledge base, a smaller chunk size is better.  Smaller than 800.\n",
    "\n",
    "<b>chunk overlap</b>:  If the paragraphs of content in your PDFs often refer to previous content in the document, like a large whitepaper, you might want to have a good size overlap.  128 or higher, this is totally up to the content.\n",
    "\n",
    "https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8efbb-ad52-4405-ab9f-a1d0d2c6cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63daf97c-4329-4eb9-a71d-d8d167cdac8e",
   "metadata": {},
   "source": [
    "### Create the vector database\n",
    "- take converted embeddings and place them into vector db\n",
    "- stored locally on prem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d7b58-24c6-4ba5-beea-74c8c7fe7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory=\"vector-db\")\n",
    "print('\\n' + 'Time to complete:')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb479809-eb88-4862-8cb3-d352c2752de7",
   "metadata": {},
   "source": [
    "### Prepare Chat model\n",
    "\n",
    "Llama2 7b chat chosen for this use case for its optimized human dialogue.  https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ecb659-43a1-49fd-9bcd-ddf0e128b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76f749-128d-46c2-b2e2-7cb763a7b3f3",
   "metadata": {},
   "source": [
    "### Choose precision version\n",
    "\n",
    "16-bit full precision will require at least 13Gb to run this notebook and memory usage will grow as the chat continues.\n",
    "\n",
    "4-bit precision is available with the help of huggingface and bitsandbytes load in parameter, simply swith the commented model line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f9a50-c55b-4677-9577-ac186e31ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## full precision, larger footprint on GPU\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"cuda\")\n",
    "\n",
    "### 4 bit precision, smaller GPU memory, must allow auto device or cuda:0\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, torch_dtype=torch.float32, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7621778-60ce-4493-ad3e-77919bcd347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967d50a-d145-4ad8-8c82-05a5039d9835",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "Used to initialize the advanced settings sliders in the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa0ee97-3c43-4949-a721-8aed3b736dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MAX_NEW_TOKENS = 2048\n",
    "DEFAULT_MAX_NEW_TOKENS = 1024\n",
    "#MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213abb5a-8706-432f-9912-f02436e7b178",
   "metadata": {},
   "source": [
    "### Chat Memory\n",
    "To have a positive, realistic chat experience the LLM needs to access a form of memory.  Memory for the LLM chat is basically a copy of the chat history that is given to the LLM as reference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420082c5-f432-4007-8feb-e3d60cdcb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### MEMORY PARAMETERS ###########\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5, ## number of interactions to keep in memory\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,  ## formats the chat_history into HumanMessage and AImessage entity list\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c7468-c1e5-4553-8573-4eca197c4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac14d2-359e-4741-892e-cba990abe040",
   "metadata": {},
   "source": [
    "### Main Process Input Function\n",
    "\n",
    "This is the function that orchestrates all the major components such as:\n",
    "- user variable input from the GUI\n",
    "- prompt template\n",
    "- pipeline setup\n",
    "- chain setup\n",
    "- response output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e5d7b-738c-41fc-af67-3770632a94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this chunk works, however it gives constant clarifying questions... annoying but the responses are pretty decent sometimes.\n",
    "def process_input(question,\n",
    "    chat_history,\n",
    "    system_prompt,\n",
    "    max_new_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "    top_k,\n",
    "    repetition_penalty\n",
    "                 ):\n",
    "\n",
    "    ### let's check and see that our gradio interface is passing the input variables as we expect\n",
    "    ### Change the values of sliders in gradio at run time to make changes to the inputs here\n",
    "    # print(\"SYS:\", system_prompt) \n",
    "    # print(\"ch:\", chat_history)\n",
    "    # print(\"MAX_NEW_TOKENS:\", max_new_tokens, \"T:\", temperature, \"P:\", top_p, \"K:\", top_k, \"REP_PEN:\", repetition_penalty)\n",
    "\n",
    "    \n",
    "    ### system prompt variable is typed in by the user in Gradio advanced settings text box and sent into process_input function\n",
    "    ### This is Llama2 prompt format \n",
    "    ### https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n",
    "\n",
    "#    llama2_prompt_template = \"\\n\\n [INST] <<SYS>>\" + system_prompt + \"<</SYS>>\\n\\n Context: {context} \\n\\n  Chat History: {chat_history} \\n\\n  Question: {question} \\n\\n[/INST]\".strip()\n",
    "\n",
    "    llama2_prompt_template = \"\\n\\n [INST] <<SYS>>\" + system_prompt + \"<</SYS>>\\n\\n Summaries: {summaries} \\n\\n  Chat History: {chat_history} \\n\\n  Question: {question}\\n\\n[/INST]\".strip()\n",
    "\n",
    "\n",
    "    PROMPT = PromptTemplate(\n",
    "#        input_variables=[\"context\", \"chat_history\", \"question\"], \n",
    "        input_variables=[\"summaries\", \"chat_history\", \"question\"], \n",
    "        template=llama2_prompt_template\n",
    "    )\n",
    "\n",
    "    ####  check to see what the prompt actually looks like\n",
    "    \n",
    "#    print(PROMPT)\n",
    "\n",
    "    ####### STREAMER FOR TEXT OUTPUT ############\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    ####### PIPELINE ARGUMENTS FOR THE LLM ############\n",
    "    ### more info at https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539\n",
    "    \n",
    "    text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=True,\n",
    "#    num_beams=2, beam search over 1 cannot be used with streamer\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "\n",
    "    ####### ATTACH PIPELINE TO LLM ############\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "\n",
    "    \n",
    "########  RETRIEVAL QA WITH SOURCES WORKS FAIRLY WELL IN OUR USE CASE\n",
    "    \n",
    "    ### this does NOT rephrase the question\n",
    "\n",
    "    ### info on db retriever settings:  https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\n",
    "    ### Maximum marginal relevance retrieval (mmr) will provide a more broad selection from more files\n",
    "    ## search kwargs integer is the max number of docs to return in the response\n",
    "    \n",
    "    ###### RETRIEVAL QA FROM CHAIN TYPE PARAMS ###########\n",
    "    qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4}),\n",
    "#        retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4}),\n",
    "        return_source_documents = True,\n",
    "        memory=memory,\n",
    "        verbose=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    ### this response format is best for retrieval QA chain with sources ###\n",
    "    ### Gradio will respond with only 2 arguments from chatbot.interface, first will always be the question, second will be history\n",
    "    \n",
    "    response = qa_chain(question, chat_history)\n",
    "\n",
    "    ##### TEST THE RESPONSE ######\n",
    "    \n",
    "#    print(response)\n",
    "#    print(response[\"chat_history\"])\n",
    "#    print(response[\"answer\"])\n",
    "\n",
    "\n",
    "    ##### TEST SOURCE DOCS lIST ######\n",
    "    \n",
    "    print(\"============================================\")\n",
    "    print(\"===============Source Documents============\")\n",
    "    print(\"============================================\")\n",
    "\n",
    "    for x in range(len(response[\"source_documents\"][0].metadata)):\n",
    "        print(response[\"source_documents\"][x].metadata)\n",
    "\n",
    "    print(\"============================================\")\n",
    "    print(\"============================================\")\n",
    "\n",
    "    #### chat history will be empty key if there is no actual history yet, run the bot a few times\n",
    "    \n",
    "#    print(response.keys())\n",
    "    # print(response[\"answer\"])\n",
    "#    print(response[\"sources\"])\n",
    "    \n",
    "    \n",
    "    ####### MANAGE OUTPUT ARRAY FROM STREAMER ###########\n",
    "    ## whatever is in streamer, the positional argument 'text', take it and join it all together\n",
    "    ## yield allows streaming in Gradio\n",
    "    \n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield \"\".join(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9388d-8f46-44d9-a153-c2b34bc8434b",
   "metadata": {},
   "source": [
    "### Build the Gradio GUI\n",
    "- Gradio is a quick, highly customizable UI package for your python applications:  https://www.gradio.app/\n",
    "- Combined with langchain, gradio can trigger multiple chains for a wide variety of user interactions.\n",
    "\n",
    "<b>NOTE</b>:  Gradio will output variables in the order they appear here in the interface object. There is no declaration of these variables explicitly in the creation of each one when it is sent to the processing function.  i.e. slider for temperature is the 3rd variable in the list.  It is passed as a positional argument, not as \"temperature\" variable explicitly.  You have to take those positional arguments that gradio passes out (from the user input at the browser) as positional input into your chat processing function.  \n",
    "\n",
    "#### Access the UI\n",
    "- The provided code forces Gradio to create a small web server on the local host the notebook is being served from\n",
    "- Gradio will provide a URL that can be used in a web browser, that must be accessed from within the same network, so you may need to access it using a jumphost.  In this case we used a Windows jump host and Chrome browser on the same network to access the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537952ee-d0b4-4bc1-a721-9cc7013f086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = gr.ChatInterface(\n",
    "    \n",
    "    ### call the main process function above\n",
    "    \n",
    "    fn=process_input, \n",
    "\n",
    "    ### format the dialogue box, add company avatar image\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(os.path.dirname(\"__file__\"), \"images/dell-logo-sm.jpg\"))),\n",
    "    ),\n",
    "\n",
    "    \n",
    "    additional_inputs=[\n",
    "        \n",
    "        gr.Textbox(label=\"Persona and role for system prompt:\", \n",
    "                   lines=3, \n",
    "                   value=\"\"\"You are a technical research assistant, you answer only in English language. Your audience appreciates technical details in your answer.\"\"\"\n",
    "                  ),\n",
    "        \n",
    "        gr.Slider(\n",
    "            label=\"Max new words (tokens)\",\n",
    "            minimum=1,\n",
    "            maximum=MAX_MAX_NEW_TOKENS,\n",
    "            step=1,\n",
    "            value=DEFAULT_MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Creativity (Temperature), higher is more creative, lower is less creative:\",\n",
    "            minimum=0.1,\n",
    "            maximum=2.0,\n",
    "            step=0.1,\n",
    "            value=0.6,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Top probable tokens (Nucleus sampling top-p), affects creativity:\",\n",
    "            minimum=0.05,\n",
    "            maximum=1.0,\n",
    "            step=0.05,\n",
    "            value=0.9,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Number of top tokens to choose from (Top-k):\",\n",
    "            minimum=1,\n",
    "            maximum=100,\n",
    "            step=1,\n",
    "            value=50,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Repetition penalty:\",\n",
    "            minimum=1.0,\n",
    "            maximum=1.99,\n",
    "            step=0.05,\n",
    "            value=1.2,\n",
    "        ),\n",
    "    ],\n",
    "    \n",
    "    stop_btn=None,\n",
    "    \n",
    "    examples=[\n",
    "        [\"Can you give me a detailed summary of the document 'h19642-Introduction-to-Apex-File-Storage-for-AWS.pdf'?\"],\n",
    "        [\"What are some solutions Dell provides for the Telecom Industry?\"],\n",
    "        [\"How does Dell APEX block storage support multiple availability zones?\"],\n",
    "        [\"Please document the process  of a 'cluster aware update' for Dell VXrail.\"],\n",
    "        [\"Would you please create a CTO advisory proposal comparing Dell Technologies storage PowerFlex solutions against HP storage solutions.\"],\n",
    "        [\"Would you please write a professional email response to John explaining the benefits of Dell Powerflex. Please be concise and in paragraph form, no lists or bullet points.\"],\n",
    "        [\"Create a new advertisement for Dell Technologies PowerEdge servers.  Please include an interesting headline and product description.  You want to persuade the target audience of IT decision makers to purchase PowerEdge servers. Include a section at the end titled Call to Action, listing next steps the readers should take.\"],\n",
    "\n",
    "    ],\n",
    "\n",
    ")\n",
    "\n",
    "###  SET GRADIO INTERFACE THEME (https://www.gradio.app/guides/theming-guide)\n",
    "\n",
    "#theme = gr.themes.Soft()\n",
    "#theme = gr.themes.Glass()\n",
    "theme = gr.themes.Default()\n",
    "\n",
    "\n",
    "### set width and margins in local css file\n",
    "### set Title in a markdown object at the top, then render the chat interface\n",
    "\n",
    "with gr.Blocks(theme=theme, css=\"style.css\") as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Retrieval Digital Assistant\n",
    "    \"\"\")\n",
    "    \n",
    "    chat_interface.render()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue(max_size=1)  ## sets up websockets for bidirectional comms and no timeouts, set a max number users in queue\n",
    "    demo.launch(share=False, debug=True, server_name=\"YOUR_SERVER_IP\", server_port=7861, allowed_paths=[\"images/dell-logo-sm.jpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905fc4e-627e-4e2e-aa3b-7731f9bc3d1c",
   "metadata": {},
   "source": [
    "### Inspiration code:\n",
    "\n",
    "https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0431b95-7718-4831-bccb-a37f456efb2b",
   "metadata": {},
   "source": [
    "### Author:\n",
    "David O'Dell - Solutions and AI Tech Marketing Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e49b85-5d9c-40ef-82cc-6c7f578695be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
