{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7dc7e39-dd9a-43e3-8d9a-f77af583004c",
   "metadata": {},
   "source": [
    "### Dell Technologies Proof of Concept - RAG chatbot with multi format data CSV, PDF, PPT with sources tabs and RAG toggle 4bit\n",
    "- Model:  Mistral 7B\n",
    "- Vector database:  Chroma db\n",
    "- Chain:  Langchain retrievalQAchain, huggingface pipeline\n",
    "- GUI:  Gradio interface (not with blocks)\n",
    "- Workload:  CSV, PPT and PDF files\n",
    "- Quantized to 4 bit\n",
    "\n",
    "Features in Additional Inputs:\n",
    "- Change persona ad hoc with adjustable system prompt\n",
    "- Change model parameters with sliders (temp., top-p, top-k, max_tokens)\n",
    "- Memory is intact and conversational using chat_history key\n",
    "- Create all types of content such as email, product description, product comparison tables etc.\n",
    "- Directly query / summarize a document given the title\n",
    "\n",
    "Note: The software and sample files are provided “as is” and are to be used only in conjunction with this POC application. They should not be used in production and are provided without warranty or guarantees. Please use them at your own discretion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef99d2-76ef-45c1-af70-bda234cb8fed",
   "metadata": {},
   "source": [
    "<img src=\"images/RAG-diagram-dell-technologies.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5902ec6",
   "metadata": {},
   "source": [
    "### Huggingface tools\n",
    "\n",
    "You will need to at least log in once to get the hub for tools and the embedding model.  After that you can comment this section out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d4d542-e238-4948-ade3-efb972c78cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/daol/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "## code to auto login to hugging face, avoid the login prompt\n",
    "# %pip install huggingface-hub==0.23.0\n",
    "\n",
    "# get your account token from https://huggingface.co/settings/tokens\n",
    "# this is a read-only test token\n",
    "\n",
    "token = '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'\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "#login(token=token, add_to_git_credential=True)\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ef44c-9f78-42de-9535-a47b9f8b7889",
   "metadata": {},
   "source": [
    "### Install python libraries and applications\n",
    "\n",
    "Using % to ensure installation into this conda environment and not OS python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b1132dd-4463-45fe-997f-2136c13464af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check installed GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0a0592-b751-4f5c-b97c-5cd12e8baef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb3e4a-af6a-4852-98a0-97afaff2924c",
   "metadata": {},
   "source": [
    "### Assign GPU environment vars and ID order\n",
    "\n",
    "NOTE:  to change which GPU you want visible, simply change the CUDA VISIBLE DEVICES ID to the GPU you prefer. \n",
    "This method guarantees no confusion or misplaced workloads on any GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88bf410b-3b71-4299-8bc9-0b6fb09f4482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## THESE VARIABLES MUST APPEAR BEFORE TORCH OR CUDA IS IMPORTED\n",
    "## set visible GPU devices and order of IDs to the PCI bus order\n",
    "## target the L40s that is on ID 1\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"   \n",
    "\n",
    "## this integer corresponds to the ID of the GPU, for multiple GPU use \"0,1,2,3\"...\n",
    "## to disable all GPUs, simply put empty quotes \"\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846fa67-69d7-478e-995e-d16a5ba98f72",
   "metadata": {},
   "source": [
    "### Investigate our GPU and CUDA environment\n",
    "\n",
    "NOTE:  If you are only using 1 single GPU in the visibility settings above, then the active CUDA device will always be 0 since it is the only GPU seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaec1edd-9c18-4b1f-a0e8-e3ed03b3141f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____Python, Pytorch, Cuda info____\n",
      "__Python VERSION: 3.12.8 (main, Dec  6 2024, 19:59:28) [Clang 18.1.8 ]\n",
      "__pyTorch VERSION: 2.5.1+cu124\n",
      "__CUDA RUNTIME API VERSION\n",
      "__CUDNN VERSION: 90100\n",
      "_____nvidia-smi GPU details____\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, NVIDIA L40S, 550.120, 46068 MiB, 1 MiB, 45589 MiB\n",
      "1, NVIDIA L40S, 550.120, 46068 MiB, 1 MiB, 45589 MiB\n",
      "_____Device assignments____\n",
      "Number CUDA Devices: 1\n",
      "Current cuda device:  0  **May not correspond to nvidia-smi ID above, check visibility parameter\n",
      "Device name:  NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "from subprocess import call\n",
    "print('_____Python, Pytorch, Cuda info____')\n",
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)\n",
    "print('__CUDA RUNTIME API VERSION')\n",
    "#os.system('nvcc --version')\n",
    "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "print('_____nvidia-smi GPU details____')\n",
    "call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "print('_____Device assignments____')\n",
    "print('Number CUDA Devices:', torch.cuda.device_count())\n",
    "print ('Current cuda device: ', torch.cuda.current_device(), ' **May not correspond to nvidia-smi ID above, check visibility parameter')\n",
    "print(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bae0d5-b6ef-425b-9e29-13393a839bcf",
   "metadata": {},
   "source": [
    "### Assign single GPU to device variable\n",
    "\n",
    "This command assigns GPU ID 0 to the DEVICE variable called \"cuda:0\" if pytorch can actually reach and speak with the GPU using cuda language.  Else it will use the cpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26efcc3-328c-49fe-9fe6-bbdbec423b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b639b742-c004-4bcd-9dd6-298a0fc499d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline, PromptTemplate\n",
    "\n",
    "### import loaders\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "\n",
    "### for embedding\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "#from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "### for langchain chain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "#from langchain.chains import ConversationalRetrievalChain\n",
    "from transformers import AutoTokenizer, pipeline, TextIteratorStreamer, AutoModelForCausalLM\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "### status bars and UI and other accessories\n",
    "import gradio as gr\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7481a10-af9f-4154-bddb-e429d4361ecf",
   "metadata": {},
   "source": [
    "### Clear GPU memory from any previous runs\n",
    "- assume Nvidia drivers installed\n",
    "- When running notebooks over and over again, often much of the memory is still in the GPU memory allocated cache.  Depending on the size of the GPU, this might cause out of memory issues during the next run.  It is advised to clear out the cache, or restart the kernel.\n",
    "- here we see multiple GPUs, the memory usage, any running processes and our CUDA version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14cfe3b9-5c65-4708-ad97-5bb3a04d2e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6559d3-b08a-4ecb-a00c-cddba9f958e2",
   "metadata": {},
   "source": [
    "### Clear the previous run vector database\n",
    "\n",
    "This is optional, the vector db will be rebuilt.  For a completely fresh run you can delete the local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4084ec3-db2b-475d-8e4b-5ea3f980879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## remove chroma vector db local db folder from previous run\n",
    "\n",
    "# !rm -rf \"db2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5cb4a-c179-450a-8a55-d17f29d7e4db",
   "metadata": {},
   "source": [
    "### Add PDF directory of files\n",
    "\n",
    "- ESG Summary (Environmental, Social and Governance) report\n",
    "- Dell leadership org chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62e497d-7740-4162-bb56-80c6845e06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir_loader = PyPDFDirectoryLoader(\"pdf-files-infohub/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283eb04c-1798-4e01-b509-a1f7e055439e",
   "metadata": {},
   "source": [
    "### Add CSV files\n",
    "\n",
    "CSV files are vectorized line by line.  100 lines of the file will equal 100 docs of vectorized data.\n",
    "\n",
    "This will load data into Langchain document object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14083e72-3232-44b1-b85b-cc919883cfe8",
   "metadata": {},
   "source": [
    "#### load events\n",
    "\n",
    "NOTE:  if you switch files you might run into a byte recognition error:  UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 3519: invalid start byte.  This is fixed with windows-1252 encoding.\n",
    "\n",
    "Your CSV files must be totally clean from any funny characters.  0xa0 is a funky space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab23172-4219-4cec-8169-eade985f81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_loader = CSVLoader(\"csv-files/events-schedule-dtw24.csv\", encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8868338-e0e0-48a4-889b-2c763d6ec093",
   "metadata": {},
   "source": [
    "#### load general info about conference\n",
    "\n",
    "NOTE:  if you switch files you might run into a byte recognition error:  UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 3519: invalid start byte.  This is fixed with windows-1252 encoding.\n",
    "\n",
    "Your CSV files must be totally clean from any funny characters.  0xa0 is a funky space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939b820b-b5c0-4e50-bac9-c88012796ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_info_loader = CSVLoader(\"csv-files/concierge-question-answer-list.csv\", encoding='windows-1252')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54accae-32df-40a1-8b37-680f0744e79e",
   "metadata": {},
   "source": [
    "### Add Powerpoint files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c571fa-5084-4a76-b61a-37e3a847051a",
   "metadata": {},
   "source": [
    "#### load powerpoint file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b677362-bd0b-45bf-8689-003c82aa7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_loader = UnstructuredPowerPointLoader(\"ppt-files/pan-dell-gen-ai-ppt-3pages.pptx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab65756-9a9e-49a0-8cb0-e2a20dab1c1f",
   "metadata": {},
   "source": [
    "### Merge all dataset contents into one set of docs\n",
    "\n",
    "NOTE:  if there is an error with the merge, especially when loading CSV files, check and see if there are any funny characters in the file like bad double quotes, or bad apostrophes, or bad ASCII characters.  Open VIM and check your file and remove any unusual characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "354c76ea-50ec-4de4-b316-0d798182b70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1586"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders.merge import MergedDataLoader\n",
    "\n",
    "loader_all = MergedDataLoader(loaders=[\n",
    "    events_loader, \n",
    "    general_info_loader, \n",
    "    pdf_dir_loader, \n",
    "    ppt_loader,\n",
    "])\n",
    "\n",
    "docs = loader_all.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf945d8-2c0d-48d3-b08f-31f71fd61d34",
   "metadata": {},
   "source": [
    "### Declare embedding model to use\n",
    "\n",
    "Use Instruct model to split text intelligently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad384a01-6147-4251-abfe-2329b7bb3f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daol/maymust-dell-example/RAG-chatbot-multiformat/.venv/lib/python3.12/site-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": DEVICE}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a0824-cb0b-476a-83aa-b667481be306",
   "metadata": {},
   "source": [
    "### Chunk text\n",
    "\n",
    "<b>chunk size large</b>:  If you want to provide large text overviews and summaries in your responses - appropriate for content creation tasks - then a large chunk size is helpful.  800 or higher.\n",
    "\n",
    "<b>chunk size small</b>:  If you are looking for specific answers based on extracted content from your knowledge base, a smaller chunk size is better.  Smaller than 800.\n",
    "\n",
    "<b>chunk overlap</b>:  If the paragraphs of content in your PDFs often refer to previous content in the document, like a large whitepaper, you might want to have a good size overlap.  128 or higher, this is totally up to the content.\n",
    "\n",
    "https://dev.to/peterabel/what-chunk-size-and-chunk-overlap-should-you-use-4338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96d8efbb-ad52-4405-ab9f-a1d0d2c6cecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1588"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=32)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63daf97c-4329-4eb9-a71d-d8d167cdac8e",
   "metadata": {},
   "source": [
    "### Create the vector database\n",
    "- take converted embeddings and place them into vector db\n",
    "- stored locally on prem\n",
    "- NOTE IF YOU GET A FILE HANDLER ERROR RELATED TO HNSWLIB do the following:\n",
    "- pip uninstall hnswlib\n",
    "- \n",
    "pip uninstall chroma-hnswli\n",
    "- \n",
    "pip install chroma-hnswlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f66d7b58-24c6-4ba5-beea-74c8c7fe7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time to complete:\n",
      "CPU times: user 33.9 s, sys: 979 ms, total: 34.9 s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectordb = Chroma.from_documents(texts, embeddings, persist_directory=\"db2\")\n",
    "print('\\n' + 'Time to complete:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98b18985-3e4d-4782-8e5d-ed7bff178e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1467404/2924755713.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=\"./db2\", embedding_function=embeddings)\n",
      "⚠️ It looks like you upgraded from a version below 0.6 and could benefit from vacuuming your database. Run chromadb utils vacuum --help for more information.\n"
     ]
    }
   ],
   "source": [
    "# # ### Load vector db if you've already created it --- comment this out and uncomment the above loader, splitter cells to create new vector db\n",
    "\n",
    "vectordb = Chroma(persist_directory=\"./db2\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb7dd6-2fea-49b6-81d9-0915f452a4e0",
   "metadata": {},
   "source": [
    "#### Get unique files embedded into vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c3d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding keys: dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'included'])\n",
      "\n",
      "Number of embedded docs: 9532\n"
     ]
    }
   ],
   "source": [
    "db = vectordb\n",
    "print(\"\\nEmbedding keys:\", db.get().keys())\n",
    "print(\"\\nNumber of embedded docs:\", len(db.get()[\"ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6233e686-c9e5-41af-8f23-b7bcc5c606f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get list of all file URLs in vector db\n",
    "\n",
    "def get_unique_files():\n",
    "    \n",
    "    db = vectordb\n",
    "    print(\"\\nEmbedding keys:\", db.get().keys())\n",
    "    print(\"\\nNumber of embedded docs:\", len(db.get()[\"ids\"]))\n",
    "    \n",
    "    unique_list = list({doc[\"source\"] for doc in db.get()[\"metadatas\"]})\n",
    "\n",
    "    print(\"\\nList of unique files in db:\\n\")\n",
    "    for unique_file in unique_list:\n",
    "        print(unique_file)\n",
    "\n",
    "    pretty_files = json.dumps(unique_list, indent=4, default=str)\n",
    "\n",
    "    return pretty_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "890185d1-16c2-44df-9248-f2bbbddb8a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_unique_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb479809-eb88-4862-8cb3-d352c2752de7",
   "metadata": {},
   "source": [
    "#### Prepare Instruct model\n",
    "\n",
    "The 'instruct' version of a has been fine-tuned to be able to follow prompted instructions. These models 'expect' to be asked to do something.  They are good at performing tasks, rather than chatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f95f9a50-c55b-4677-9577-ac186e31ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71be0b-fd0c-4671-9488-3f1bc7e304d7",
   "metadata": {},
   "source": [
    "#### Quantization Configuration\n",
    "Great video on this:  https://www.youtube.com/watch?v=eovBbABk3hw&ab_channel=Rohan-Paul-AI\n",
    "\n",
    "Bitsandbytes stores weights in 4 bits, the computations still happen in 16 or 32 bit depending on bfloat choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65175e6f-c8c5-4d85-a73b-0a71111cac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f53199ab-8f74-4cbf-b1df-c5d3af9de352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3d5a28ef724c88bbfc97fe21e4784e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "#    load_in_4bit=True,\n",
    "#    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16836a2b-c7dd-414f-a027-dbaa938cf02e",
   "metadata": {},
   "source": [
    "#### Initialize tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "972bd073-54ea-429e-a86b-2e26e2aab9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id) #! AutoTokenizer cannot correctly identify LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b596d6ad-f0d2-4ae2-9456-c74b185d97d5",
   "metadata": {},
   "source": [
    "#### Check GPU memory usage after model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49e4dbbc-9b1c-4b23-b83c-ef4c69244f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid, process_name, used_gpu_memory [MiB]\n",
      "1467404, /home/daol/maymust-dell-example/RAG-chatbot-multiformat/.venv/bin/python, 6382 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785c93a-5465-47b9-8c64-0b578166ef3f",
   "metadata": {},
   "source": [
    "#### Print interesting metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3c2294c-7ada-4c4b-81b6-eca2341ce9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_info ():\n",
    "\n",
    "    model_details = (\n",
    "    \n",
    "    f\"\\nGeneral Model Info:\\n\"\n",
    "    f\"\\n-------------------\\n\"\n",
    "    \n",
    "    f\"\\n Model_id: {model_id} \\n\"\n",
    "    f\"\\n Model config: {model} \\n\"\n",
    "\n",
    "    f\"\\nGeneral Embeddings Info:\\n\"\n",
    "    f\"\\n-------------------\\n\"\n",
    "\n",
    "    f\"\\n Embeddings model config: {embeddings} \\n\" \n",
    "\n",
    "    )\n",
    "        \n",
    "    return model_details\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "895bb115-2330-4a55-84c6-28fbba31ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4967d50a-d145-4ad8-8c82-05a5039d9835",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "Used to initialize the advanced settings sliders in the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fa0ee97-3c43-4949-a721-8aed3b736dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MAX_NEW_TOKENS = 2048\n",
    "DEFAULT_MAX_NEW_TOKENS = 1024\n",
    "#MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213abb5a-8706-432f-9912-f02436e7b178",
   "metadata": {},
   "source": [
    "### Chat Memory\n",
    "To have a positive, realistic chat experience the LLM needs to access a form of memory.  Memory for the LLM chat is basically a copy of the chat history that is given to the LLM as reference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "420082c5-f432-4007-8feb-e3d60cdcb685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1467404/3561771124.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(\n"
     ]
    }
   ],
   "source": [
    "####### MEMORY PARAMETERS ###########\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5, ## number of interactions to keep in memory\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,  ## formats the chat_history into HumanMessage and AImessage entity list\n",
    "    input_key=\"query\",   ### for straight retrievalQA chain\n",
    "    output_key=\"result\"   ### for straight retrievalQA chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac14d2-359e-4741-892e-cba990abe040",
   "metadata": {},
   "source": [
    "### Main Process Input Function\n",
    "\n",
    "This is the function that orchestrates all the major components such as:\n",
    "- user variable input from the GUI\n",
    "- prompt template\n",
    "- pipeline setup\n",
    "- chain setup\n",
    "- response output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "347e5d7b-738c-41fc-af67-3770632a94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this chunk works, however it gives constant clarifying questions... annoying but the responses are pretty decent sometimes.\n",
    "def process_input(\n",
    "    question,\n",
    "    chat_history,\n",
    "    rag_toggle,\n",
    "    system_prompt,\n",
    "    source_docs_qty,\n",
    "    max_new_tokens,\n",
    "    temperature,\n",
    "    top_p,\n",
    "    top_k,\n",
    "    repetition_penalty\n",
    "                 ):\n",
    "\n",
    "#     print(\"1\", question)\n",
    "#     print(\"2\", chat_history)\n",
    "#     print(\"3\", rag_toggle)\n",
    "#     print(\"4\", system_prompt)\n",
    "#     print(\"5\", source_docs_qty)\n",
    "    \n",
    "    \n",
    "    \n",
    "    global response\n",
    "\n",
    "    \n",
    "    ### system prompt variable is typed in by the user in Gradio advanced settings text box and sent into process_input function\n",
    "    ### This is Llama2 prompt format \n",
    "    ### https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n",
    "    \n",
    "    prompt_template_rag = \"\\n\\n [INST] <<SYS>>\" + system_prompt + \"<</SYS>>\\n\\n Context: {context} \\n\\n  Question: {question} \\n\\n[/INST]\".strip()\n",
    "\n",
    "\n",
    "    PROMPT_rag = PromptTemplate(template=prompt_template_rag, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "\n",
    "    prompt_template_llm = \"\\n\\n [INST] <<SYS>>\" + system_prompt + \"<</SYS>>\\n\\n Question: {question} \\n\\n[/INST]\".strip()\n",
    "\n",
    "\n",
    "    PROMPT_llm = PromptTemplate(template=prompt_template_llm, input_variables=[\"question\"])\n",
    "    \n",
    "        \n",
    "    \n",
    "    ####### STREAMER FOR TEXT OUTPUT ############\n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    ####### PIPELINE ARGUMENTS FOR THE LLM ############\n",
    "    ### more info at https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539\n",
    "    \n",
    "    text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    do_sample=True,\n",
    "    streamer=streamer,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    top_p=top_p,\n",
    "    top_k=top_k,\n",
    "    temperature=temperature,\n",
    "    repetition_penalty=repetition_penalty,\n",
    "    )\n",
    "\n",
    "    ####### ATTACH PIPELINE TO LLM ############\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "        \n",
    "\n",
    "    llmchain = LLMChain(llm=llm, prompt=PROMPT_llm)\n",
    "\n",
    "\n",
    "    ###### RETRIEVAL QA FROM CHAIN TYPE PARAMS ###########\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": PROMPT_rag},\n",
    "#        retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}),\n",
    "        retriever=vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": source_docs_qty}),\n",
    "        memory=memory,\n",
    "        verbose=True,\n",
    "        return_source_documents = True,\n",
    "        )\n",
    "\n",
    "    \n",
    "    \n",
    "    #########################\n",
    "    if rag_toggle:\n",
    "    \n",
    "        response = qa_chain({\"query\": question})\n",
    "\n",
    "    else:\n",
    "    \n",
    "        response = llmchain({\"question\": question})\n",
    "        \n",
    "\n",
    "    #########################    \n",
    "    \n",
    "\n",
    "    ####### MANAGE OUTPUT ARRAY FROM STREAMER ###########\n",
    "    ## whatever is in streamer, the positional argument 'text', take it and join it all together\n",
    "    ## yield allows streaming in Gradio\n",
    "    \n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield \"\".join(outputs)\n",
    "\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbf5261-b7c1-4ca1-8980-81a92b15cad1",
   "metadata": {},
   "source": [
    "### Show sources function\n",
    "\n",
    "Sources are critical to demonstrate the LLMs response is true to the source and not hallucinating.  This function using the global \"response\" variable created in the process_input function. This content is parsed with jsondumps and shown in the GUI textbox at the very bottom. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca762d46-d8ae-4963-8e8c-dad9127befda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sources():\n",
    "\n",
    "    res_dict = {\n",
    "        \"answer_from_llm\": response[\"result\"],   ### looks up result key from raw output\n",
    "    }\n",
    "    \n",
    "    res_dict[\"source_documents\"] = []    ### create an empty array for source documents key front result dict\n",
    "\n",
    "    for each_source in response[\"source_documents\"]:\n",
    "        res_dict[\"source_documents\"].append({\n",
    "            \"page_content\": each_source.page_content,\n",
    "            \"metadata\":  each_source.metadata\n",
    "        })\n",
    "\n",
    "    # print(res_dict[\"answer_from_llm\"])  ### PRINT JUST THE RAW ANSWER FROM LLM\n",
    "    \n",
    "    pretty_sources = json.dumps(res_dict[\"source_documents\"], indent=4, default=str)\n",
    "\n",
    "    print(pretty_sources)\n",
    "    \n",
    "    return pretty_sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f9388d-8f46-44d9-a153-c2b34bc8434b",
   "metadata": {},
   "source": [
    "### Build the Gradio GUI\n",
    "- Gradio is a quick, highly customizable UI package for your python applications:  https://www.gradio.app/\n",
    "- Combined with langchain, gradio can trigger multiple chains for a wide variety of user interactions.\n",
    "\n",
    "<b>NOTE</b>:  Gradio will output variables in the order they appear here in the interface object. There is no declaration of these variables explicitly in the creation of each one when it is sent to the processing function.  i.e. slider for temperature is the 3rd variable in the list.  It is passed as a positional argument, not as \"temperature\" variable explicitly.  You have to take those positional arguments that gradio passes out (from the user input at the browser) as positional input into your chat processing function.  \n",
    "\n",
    "#### Access the UI\n",
    "- The provided code forces Gradio to create a small web server on the local host the notebook is being served from\n",
    "- Gradio will provide a URL that can be used in a web browser, that must be accessed from within the same network, so you may need to access it using a jumphost.  In this case we used a Windows jump host and Chrome browser on the same network to access the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "537952ee-d0b4-4bc1-a721-9cc7013f086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = gr.ChatInterface(\n",
    "    \n",
    "    ### call the main process function above\n",
    "    \n",
    "    fn=process_input, \n",
    "\n",
    "    ### format the dialogue box, add company avatar image\n",
    "    \n",
    "    chatbot = gr.Chatbot(\n",
    "        bubble_full_width=False,\n",
    "        avatar_images=(None, (os.path.join(os.path.dirname(\"__file__\"), \"images/dell-logo-sm.jpg\"))),\n",
    "    ),\n",
    "\n",
    "    \n",
    "    \n",
    "    additional_inputs=[\n",
    "        \n",
    "        \n",
    "        gr.Checkbox(label=\"Use RAG\", \n",
    "                    value=True, \n",
    "                    info=\"Query LLM directly or query the RAG chain\"\n",
    "                    ),\n",
    "        \n",
    "        \n",
    "        gr.Textbox(label=\"Persona and role for system prompt:\", \n",
    "                lines=3, \n",
    "                value=\"\"\"Your name is Andie, a helpful concierge at the Dell Tech World conference held in Las Vegas.\\\n",
    "                Please respond as if you were talking to someone using spoken English language.\\\n",
    "                The first word of your response should never be Answer:.\\\n",
    "                You are given a list of helpful information about the conference.\\\n",
    "                Your goal is to use the given information to answer attendee questions.\\\n",
    "                Please do not provide any additional information other than what is needed to directly answer the question.\\\n",
    "                You do not need to show or refer to your sources in your responses.\\\n",
    "                Please do not make up information that is not available from the given data.\\\n",
    "                If you can't find the specific information from the given context, please say that you don't know.\\\n",
    "                Please respond in a helpful, concise manner.\\\n",
    "                \"\"\"\n",
    "\n",
    "                ),\n",
    "\n",
    "        gr.Slider(\n",
    "            label=\"Number of source docs\",\n",
    "            minimum=1,\n",
    "            maximum=10,\n",
    "            step=1,\n",
    "            value=3,\n",
    "        ),\n",
    "        \n",
    "        gr.Slider(\n",
    "            label=\"Max new words (tokens)\",\n",
    "            minimum=1,\n",
    "            maximum=MAX_MAX_NEW_TOKENS,\n",
    "            step=1,\n",
    "            value=DEFAULT_MAX_NEW_TOKENS,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Creativity (Temperature), higher is more creative, lower is less creative:\",\n",
    "            minimum=0.1,\n",
    "            maximum=1.99,\n",
    "            step=0.1,\n",
    "            value=0.6,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Top probable tokens (Nucleus sampling top-p), affects creativity:\",\n",
    "            minimum=0.05,\n",
    "            maximum=1.0,\n",
    "            step=0.05,\n",
    "            value=0.9,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Number of top tokens to choose from (Top-k):\",\n",
    "            minimum=1,\n",
    "            maximum=100,\n",
    "            step=1,\n",
    "            value=50,\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            label=\"Repetition penalty:\",\n",
    "            minimum=1.0,\n",
    "            maximum=1.99,\n",
    "            step=0.05,\n",
    "            value=1.2,\n",
    "        ),\n",
    "    ],\n",
    "\n",
    "    \n",
    "    stop_btn=None,\n",
    "    \n",
    "    examples=[\n",
    "\n",
    "        ## events csv content\n",
    "        [\"Which booths are found in the showcase floor at Dell Technologies World 2024?\"],\n",
    "        [\"What are some common use cases for GenAI?\"],\n",
    "        [\"Where is the Charting the Generative AI landscape in healthcare session going to be held?\"],\n",
    "        [\"Who is hosting the Understanding GenAI as a workload in a multicloud world session?\"],\n",
    "        [\"What enterprise Retrieval Augmented Generation solutions does Dell offer?\"],\n",
    "\n",
    "        ## Powerpoint content\n",
    "        [\"What are some of the results of the Dell Generative AI Pulse Survey?\"],\n",
    "        \n",
    "\n",
    "        ## pdf content, content creation, workplace productivity\n",
    "        [\"What is Dell's ESG policy in one sentence?\"],\n",
    "        [\"Would you please write a professional email response to John explaining the benefits of Dell Powerflex.\"],\n",
    "        [\"Create a new advertisement for Dell Technologies PowerEdge servers. Please include an interesting headline and product description.\"],\n",
    "        [\"Create 3 engaging tweets highlighting the key advantages of using Dell Technologies solutions for Generative AI.\"],\n",
    "        [\"What are the key steps in designing a secure and scalable on-premises solution for GenAI workloads with Dell?\"],\n",
    "        [\"Summarize the significant developments from Dell's latest SEC filings.\"],\n",
    "\n",
    "    ],\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7d4fefd-d2b6-4242-871a-64476aea55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  SET GRADIO INTERFACE THEME (https://www.gradio.app/guides/theming-guide)\n",
    "\n",
    "#theme = gr.themes.Soft()\n",
    "#theme = gr.themes.Glass()\n",
    "#theme = gr.themes.Base()\n",
    "\n",
    "theme = gr.themes.Default()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d8dca-5b38-41e7-a963-8d3cf08a3960",
   "metadata": {},
   "source": [
    "#### Tabbed interfaces one for chat one for sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9f24826-8e4b-4def-9b59-2d2220f3155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set width and margins in local css file\n",
    "### set Title in a markdown object at the top, then render the chat interface\n",
    "\n",
    "with gr.Blocks(theme=theme, css=\"style.css\", title=\"RAG Chat CSV PDF PPT\") as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Retrieval Digital Assistant\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Tab(\"Chat Session\"):\n",
    "\n",
    "        chat_interface.render()\n",
    "\n",
    "    with gr.Tab(\"Source Citations\"):\n",
    "            \n",
    "        source_text_box = gr.Textbox(label=\"Reference Sources\")\n",
    "        get_source_button = gr.Button(\"Get Source Content\")\n",
    "        get_source_button.click(fn=get_sources, inputs=None, outputs=source_text_box)\n",
    "\n",
    "\n",
    "    with gr.Tab(\"Database Files\"):\n",
    "\n",
    "\n",
    "        files_text_box = gr.Textbox(label=\"Uploaded Files\")\n",
    "        get_files_button = gr.Button(\"List Uploaded Files\")\n",
    "        get_files_button.click(fn=get_unique_files, inputs=None, outputs=files_text_box)\n",
    "\n",
    "\n",
    "    with gr.Tab(\"Model Info\"):\n",
    "\n",
    "\n",
    "        model_info_text_box = gr.Textbox(label=\"Model Info\")\n",
    "        model_info_button = gr.Button(\"Get Model Info\")\n",
    "        model_info_button.click(fn=get_model_info, inputs=None, outputs=model_info_text_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fff57-f053-4895-b8eb-65e54ddc0b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://localhost:7890\n",
      "Running on public URL: https://092e14ea9d86d722d5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://092e14ea9d86d722d5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1467404/3078217279.py:65: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
      "/tmp/ipykernel_1467404/3078217279.py:68: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llmchain = LLMChain(llm=llm, prompt=PROMPT_llm)\n",
      "/tmp/ipykernel_1467404/3078217279.py:88: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"query\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo.queue(max_size=5)  ## sets up websockets for bidirectional comms and no timeouts, set a max number users in queue\n",
    "    demo.launch(share=True, debug=True, server_name=\"localhost\", server_port=7890, allowed_paths=[\"images/dell-logo-sm.jpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2905fc4e-627e-4e2e-aa3b-7731f9bc3d1c",
   "metadata": {},
   "source": [
    "### Inspiration code:\n",
    "\n",
    "https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0431b95-7718-4831-bccb-a37f456efb2b",
   "metadata": {},
   "source": [
    "### Author:\n",
    "David O'Dell - Solutions and AI Tech Marketing Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e49b85-5d9c-40ef-82cc-6c7f578695be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda env export --name rag | grep -v \"^prefix: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c817cd-7b90-41f5-9902-6b32bc497379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
